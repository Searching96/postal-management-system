# SE363 ‚Äì Ph√°t tri·ªÉn ·ª©ng d·ª•ng tr√™n n·ªÅn t·∫£ng d·ªØ li·ªáu l·ªõn
# Khoa C√¥ng ngh·ªá Ph·∫ßn m·ªÅm ‚Äì Tr∆∞·ªùng ƒê·∫°i h·ªçc C√¥ng ngh·ªá Th√¥ng tin, ƒêHQG-HCM
# HopDT ‚Äì Faculty of Software Engineering, University of Information Technology (FSE-UIT)

# consumer_postgres_streaming_batch.py
# ======================================
# Consumer ƒë·ªçc d·ªØ li·ªáu t·ª´ Kafka topic "absa-reviews"
# ‚Üí T√≠ch l≈©y ƒë·∫øn 100 comments
# ‚Üí Ch·∫°y batch inference m√¥ h√¨nh ABSA (.pt)
# ‚Üí Ghi k·∫øt qu·∫£ v√†o PostgreSQL

from pyspark.sql import SparkSession, functions as F, types as T
from pyspark.sql.functions import pandas_udf, from_json, col, count, window
import pandas as pd, torch, torch.nn as nn, torch.nn.functional as tF
from transformers import AutoTokenizer, AutoModel
import random, time, os, sys, json

# === 1. Spark session v·ªõi Kafka connector ===
scala_version = "2.12"
spark_version = "3.5.1"

spark = (
    SparkSession.builder
    .appName("Kafka_ABSA_Postgres_Batch")
    .config("spark.jars.packages",
            f"org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version},"
            "org.postgresql:postgresql:42.6.0,"
            "org.apache.kafka:kafka-clients:3.6.1")
    .config("spark.sql.streaming.checkpointLocation", "/opt/airflow/checkpoints/absa_streaming_checkpoint")
    .getOrCreate()
)
spark.sparkContext.setLogLevel("WARN")

# === 2. ƒê·ªçc d·ªØ li·ªáu streaming t·ª´ Kafka ===
df_stream = (
    spark.readStream
    .format("kafka")
    .option("kafka.bootstrap.servers", "kafka:9092")
    .option("subscribe", "absa-reviews")
    .option("startingOffsets", "latest")
    .option("maxOffsetsPerTrigger", 10)  # ƒê·ªçc t·ªëi ƒëa 10 messages m·ªói l·∫ßn trigger
    .load()
)

df_text = df_stream.selectExpr("CAST(value AS STRING) as Review")

# === 3. ƒê·ªãnh nghƒ©a m√¥ h√¨nh ABSA ===
ASPECTS = ["Price","Shipping","Outlook","Quality","Size","Shop_Service","General","Others"]
MODEL_NAME = "xlm-roberta-base"
MODEL_PATH = "/opt/airflow/models/best_absa_hardshare.pt"
MAX_LEN = 64
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

_model, _tokenizer = None, None

class ABSAModel(nn.Module):
    def __init__(self, roberta, n_aspects=8):
        super().__init__()
        self.roberta = roberta
        hidden = roberta.config.hidden_size
        self.classifier = nn.Linear(hidden, n_aspects * 4)

    def forward(self, input_ids, attention_mask):
        out = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
        pooled = out.last_hidden_state[:, 0]
        return self.classifier(pooled)

def load_model():
    global _model, _tokenizer
    if _model is None:
        print(f"[ABSA] Loading model from {MODEL_PATH}...")
        _tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        base_model = AutoModel.from_pretrained(MODEL_NAME)
        _model = ABSAModel(base_model, len(ASPECTS))
        
        if os.path.exists(MODEL_PATH):
            state = torch.load(MODEL_PATH, map_location=DEVICE)
            _model.load_state_dict(state)
            print("[ABSA] ‚úÖ Model loaded successfully.")
        else:
            print("[ABSA] ‚ö†Ô∏è Model file not found ‚Äî using random weights for demo.")
        
        _model.to(DEVICE)
        _model.eval()
    return _model, _tokenizer

# === 4. UDF cho inference (batch processing) ===
@pandas_udf(T.ArrayType(T.FloatType()))
def absa_infer_udf(texts: pd.Series) -> pd.Series:
    model, tokenizer = load_model()
    
    results = []
    with torch.no_grad():
        for txt in texts:
            if not txt or txt.strip() == "":
                results.append([0.0] * (len(ASPECTS) * 4))
                continue
            
            enc = tokenizer(txt, return_tensors="pt", max_length=MAX_LEN, 
                          truncation=True, padding="max_length")
            enc = {k: v.to(DEVICE) for k, v in enc.items()}
            logits = model(**enc)
            probs = tF.softmax(logits.reshape(-1, 4), dim=1)
            results.append(probs.flatten().cpu().tolist())
    
    return pd.Series(results)

df_pred = df_text.withColumn("predictions", absa_infer_udf(F.col("Review")))

@pandas_udf("string")
def decode_sentiment(preds: pd.Series) -> pd.Series:
    SENTIMENTS = ["POS", "NEU", "NEG"]
    res = []
    for p in preds:
        if not p:
            res.append("?")
            continue
        p = list(p)
        p_m, p_s = p[:len(ASPECTS)], p[len(ASPECTS):]
        decoded = []
        for i, asp in enumerate(ASPECTS):
            triplet = p_s[i*3:(i+1)*3]
            s = SENTIMENTS[int(max(range(3), key=lambda j: triplet[j]))]
            decoded.append(f"{asp}:{s}")
        res.append(", ".join(decoded))
    return pd.Series(res)

df_final = df_pred.withColumn("decoded", decode_sentiment(F.col("predictions")))
for asp in ASPECTS:
    df_final = df_final.withColumn(asp, F.regexp_extract("decoded", f"{asp}:(\\w+)", 1))

# === Gi·∫£i m√£ Review JSON th√†nh text ti·∫øng Vi·ªát tr∆∞·ªõc khi stream ===
review_schema = T.StructType([
    T.StructField("id", T.StringType()),
    T.StructField("review", T.StringType())
])
df_final = df_final.withColumn("ReviewText", from_json(col("Review"), review_schema).getField("review"))

# === 5. Ghi k·∫øt qu·∫£ v√†o PostgreSQL v·ªõi batch 128 records ===
batch_buffer = []
BATCH_SIZE = 128

def write_to_postgres(batch_df, batch_id):
    """
    T√≠ch l≈©y ƒë·∫øn 128 comments r·ªìi m·ªõi predict v√† ghi v√†o DB
    """
    global batch_buffer
    
    sys.stdout.reconfigure(encoding='utf-8')
    total_rows = batch_df.count()

    if total_rows == 0:
        print(f"[Batch {batch_id}] ‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi.")
        return

    # Thu th·∫≠p d·ªØ li·ªáu v√†o buffer
    batch_data = batch_df.select("ReviewText", *ASPECTS).collect()
    batch_buffer.extend(batch_data)
    
    print(f"[Batch {batch_id}] Nh·∫≠n {total_rows} records. Buffer hi·ªán t·∫°i: {len(batch_buffer)}/{BATCH_SIZE}")

    # Ch·ªâ x·ª≠ l√Ω khi ƒë·ªß BATCH_SIZE
    if len(batch_buffer) < BATCH_SIZE:
        print(f"[Batch {batch_id}] ‚è≥ Ch∆∞a ƒë·ªß {BATCH_SIZE} comments, ch·ªù th√™m...")
        return
    
    # L·∫•y ƒë√∫ng BATCH_SIZE records ƒë·ªÉ x·ª≠ l√Ω
    records_to_process = batch_buffer[:BATCH_SIZE]
    batch_buffer = batch_buffer[BATCH_SIZE:]  # Gi·ªØ l·∫°i ph·∫ßn c√≤n l·∫°i
    
    print(f"[Batch {batch_id}] ‚úÖ ƒê·ªß {BATCH_SIZE} comments! B·∫Øt ƒë·∫ßu predict v√† ghi v√†o PostgreSQL...")
    
    # Chuy·ªÉn v·ªÅ DataFrame ƒë·ªÉ ghi
    df_to_write = spark.createDataFrame(records_to_process)
    
    preview = df_to_write.limit(5).toPandas().to_dict(orient="records")
    print(f"[Batch {batch_id}] Preview 5 records ƒë·∫ßu:")
    print(json.dumps(preview, ensure_ascii=False, indent=2))

    try:
        (df_to_write
            .write
            .format("jdbc")
            .option("url", "jdbc:postgresql://postgres:5432/airflow")
            .option("dbtable", "absa_results")
            .option("user", "airflow")
            .option("password", "airflow")
            .option("driver", "org.postgresql.Driver")
            .option("charset", "utf8")
            .mode("append")
            .save()
        )
        print(f"[Batch {batch_id}] ‚úÖ Ghi {len(records_to_process)} records v√†o PostgreSQL th√†nh c√¥ng!")

    except Exception as e:
        print(f"[Batch {batch_id}] ‚ö†Ô∏è Kh√¥ng th·ªÉ ghi v√†o PostgreSQL: {str(e)}")
        # Log ra console thay th·∫ø
        subset = df_to_write.limit(5).toPandas().to_dict(orient="records")
        print(json.dumps(subset, ensure_ascii=False, indent=2))

# === 6. B·∫Øt ƒë·∫ßu stream v·ªõi trigger d√†i h∆°n ƒë·ªÉ t√≠ch l≈©y d·ªØ li·ªáu ===
query = (
    df_final.writeStream
    .foreachBatch(write_to_postgres)
    .outputMode("append")
    .trigger(processingTime="10 seconds")  # Trigger m·ªói 10 gi√¢y
    .start()
)

print(f"üöÄ Batch Streaming job started ‚Äî t√≠ch l≈©y {BATCH_SIZE} comments tr∆∞·ªõc khi predict...")
print(f"üìä M·ªói {BATCH_SIZE} comments s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω c√πng l√∫c v√† ghi v√†o PostgreSQL.")
query.awaitTermination()
