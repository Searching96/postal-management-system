# SE363 ‚Äì Ph√°t tri·ªÉn ·ª©ng d·ª•ng tr√™n n·ªÅn t·∫£ng d·ªØ li·ªáu l·ªõn
# Khoa C√¥ng ngh·ªá Ph·∫ßn m·ªÅm ‚Äì Tr∆∞·ªùng ƒê·∫°i h·ªçc C√¥ng ngh·ªá Th√¥ng tin, ƒêHQG-HCM
# HopDT ‚Äì Faculty of Software Engineering, University of Information Technology (FSE-UIT)

# comment_api.py
# ======================================
# Flask API nh·∫≠n comment t·ª´ Spring Boot backend
# v√† g·ª≠i v√†o Kafka topic "absa-reviews" ƒë·ªÉ consumer x·ª≠ l√Ω

from flask import Flask, request, jsonify
from kafka import KafkaProducer
from kafka.admin import KafkaAdminClient, NewTopic
from kafka.errors import TopicAlreadyExistsError
import json
import logging
import time
import redis
import os

# === C·∫•u h√¨nh ===
app = Flask(__name__)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

KAFKA_SERVER = "postal-kafka:9092"
TOPIC = "absa-reviews"
REDIS_HOST = os.getenv("REDIS_HOST", "postal-redis")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
REDIS_TTL = 3600  # K·∫øt qu·∫£ t·ªìn t·∫°i 1 gi·ªù

# === K·∫øt n·ªëi Redis ===
redis_client = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)

# === Kh·ªüi t·∫°o Kafka Producer ===
producer = None

def init_kafka():
    """Kh·ªüi t·∫°o Kafka producer v√† t·∫°o topic n·∫øu ch∆∞a t·ªìn t·∫°i"""
    global producer
    
    # ƒê·ª£i Kafka kh·ªüi ƒë·ªông
    max_retries = 30
    for i in range(max_retries):
        try:
            # T·∫°o topic n·∫øu ch∆∞a c√≥
            admin = KafkaAdminClient(bootstrap_servers=KAFKA_SERVER, client_id="comment_api_admin")
            topic = NewTopic(name=TOPIC, num_partitions=1, replication_factor=1)
            admin.create_topics([topic])
            logger.info(f"‚úÖ Created topic: {TOPIC}")
            admin.close()
            break
        except TopicAlreadyExistsError:
            logger.info(f"‚ÑπÔ∏è Topic {TOPIC} already exists")
            break
        except Exception as e:
            if i < max_retries - 1:
                logger.warning(f"‚è≥ Waiting for Kafka... ({i+1}/{max_retries})")
                time.sleep(2)
            else:
                logger.error(f"‚ùå Failed to connect to Kafka: {e}")
                raise
    
    # Kh·ªüi t·∫°o Producer
    producer = KafkaProducer(
        bootstrap_servers=KAFKA_SERVER,
        value_serializer=lambda v: json.dumps(v).encode("utf-8"),
        acks='all',  # ƒê·∫£m b·∫£o message ƒë∆∞·ª£c ghi th√†nh c√¥ng
        retries=3
    )
    logger.info("‚úÖ Kafka Producer initialized successfully")

# === API Endpoints ===

@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({"status": "healthy", "service": "comment-api"}), 200

@app.route('/api/comments', methods=['POST'])
def receive_comment():
    """
    Nh·∫≠n OrderComment t·ª´ Spring Boot v√† g·ª≠i v√†o Kafka
    
    Expected JSON format (t·ª´ OrderComment entity):
    {
        "id": 123,  // Order comment ID
        "comment_text": "S·∫£n ph·∫©m t·ªët! Giao h√†ng nhanh.",
        "callback_url": "http://your-backend:8080/api/callbacks/absa-result"  // optional
    }
    """
    try:
        # Validate request
        if not request.is_json:
            return jsonify({"error": "Content-Type must be application/json"}), 400
        
        data = request.get_json()
        
        # Validate required fields
        if 'comment_text' not in data:
            return jsonify({"error": "Missing required field: comment_text"}), 400
        
        if 'id' not in data:
            return jsonify({"error": "Missing required field: id"}), 400
        
        # Chu·∫©n b·ªã message ƒë·ªÉ g·ª≠i v√†o Kafka
        comment_id = str(data["id"])  # Convert to string for consistency
        message = {
            "id": comment_id,
            "comment_text": data["comment_text"],
            "timestamp": data.get("timestamp", time.strftime("%Y-%m-%dT%H:%M:%S")),
            "callback_url": data.get("callback_url")  # Optional callback URL
        }
        
        # G·ª≠i v√†o Kafka
        future = producer.send(TOPIC, message)
        
        # ƒê·ª£i x√°c nh·∫≠n (blocking, nh∆∞ng timeout nhanh)
        record_metadata = future.get(timeout=10)
        
        # Push v√†o Redis buffer cho batch inference
        redis_client.rpush("absa:buffer", json.dumps(message))
        
        logger.info(f"‚úÖ Sent OrderComment to Kafka + Redis: {comment_id}")
        
        return jsonify({
            "success": True,
            "message": "OrderComment received and queued for processing",
            "order_comment_id": comment_id,
            "kafka_partition": record_metadata.partition,
            "kafka_offset": record_metadata.offset,
            "status_url": f"/api/results/{comment_id}"  # URL ƒë·ªÉ check k·∫øt qu·∫£
        }), 201
        
    except Exception as e:
        logger.error(f"‚ùå Error processing comment: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

@app.route('/api/comments/batch', methods=['POST'])
def receive_comments_batch():
    """
    Nh·∫≠n nhi·ªÅu OrderComments c√πng l√∫c t·ª´ Spring Boot
    
    Expected JSON format:
    {
        "comments": [
            {"id": 1, "comment_text": "S·∫£n ph·∫©m t·ªët!", "callback_url": "http://..."},
            {"id": 2, "comment_text": "Giao h√†ng nhanh!"}
        ]
    }
    """
    try:
        if not request.is_json:
            return jsonify({"error": "Content-Type must be application/json"}), 400
        
        data = request.get_json()
        
        if 'comments' not in data or not isinstance(data['comments'], list):
            return jsonify({"error": "Missing or invalid 'comments' array"}), 400
        
        success_count = 0
        failed_count = 0
        comment_ids = []
        
        for comment_data in data['comments']:
            try:
                if 'comment_text' not in comment_data or 'id' not in comment_data:
                    failed_count += 1
                    continue
                
                comment_id = str(comment_data["id"])
                message = {
                    "id": comment_id,
                    "comment_text": comment_data["comment_text"],
                    "timestamp": comment_data.get("timestamp", time.strftime("%Y-%m-%dT%H:%M:%S")),
                    "callback_url": comment_data.get("callback_url")
                }
                
                producer.send(TOPIC, message)
                # Push v√†o Redis buffer cho batch inference
                redis_client.rpush("absa:buffer", json.dumps(message))
                comment_ids.append(comment_id)
                success_count += 1
                
            except Exception as e:
                logger.error(f"Failed to send comment: {str(e)}")
                failed_count += 1
        
        producer.flush()  # ƒê·∫£m b·∫£o t·∫•t c·∫£ messages ƒë∆∞·ª£c g·ª≠i
        
        logger.info(f"‚úÖ Batch sent: {success_count} success, {failed_count} failed")
        
        return jsonify({
            "success": True,
            "sent": success_count,
            "failed": failed_count,
            "comment_ids": comment_ids
        }), 201
        
    except Exception as e:
        logger.error(f"‚ùå Error processing batch: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

@app.route('/api/results/<comment_id>', methods=['GET'])
def get_result(comment_id):
    """
    L·∫•y k·∫øt qu·∫£ predict cho m·ªôt OrderComment ID c·ª• th·ªÉ
    
    Response:
    {
        "success": true,
        "order_comment_id": "123",
        "status": "completed",  // pending, completed, failed
        "result": {
            "comment_text": "...",
            "predictions": {
                "Price": "POS",
                "Shipping": "NEU",
                ...
            }
        }
    }
    """
    try:
        # L·∫•y t·ª´ Redis
        result_json = redis_client.get(f"result:{comment_id}")
        
        if result_json:
            result = json.loads(result_json)
            return jsonify({
                "success": True,
                "order_comment_id": comment_id,
                "status": "completed",
                "result": result
            }), 200
        else:
            # Ch∆∞a c√≥ k·∫øt qu·∫£
            return jsonify({
                "success": True,
                "order_comment_id": comment_id,
                "status": "pending",
                "message": "Result not yet available. Batch processing in progress."
            }), 202  # 202 Accepted
            
    except Exception as e:
        logger.error(f"‚ùå Error getting result: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

@app.route('/api/results/batch', methods=['POST'])
def get_results_batch():
    """
    L·∫•y k·∫øt qu·∫£ cho nhi·ªÅu comment IDs
    
    Request:
    {
        "comment_ids": ["comment_1", "comment_2", "comment_3"]
    }
    """
    try:
        data = request.get_json()
        comment_ids = data.get('comment_ids', [])
        
        results = {}
        for comment_id in comment_ids:
            result_json = redis_client.get(f"result:{comment_id}")
            if result_json:
                results[comment_id] = json.loads(result_json)
            else:
                results[comment_id] = {"status": "pending"}
        
        return jsonify({
            "success": True,
            "results": results
        }), 200
        
    except Exception as e:
        logger.error(f"‚ùå Error getting batch results: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500


# === Batch Status APIs ===

BATCH_SIZE = 128
BATCH_TIMEOUT_HOURS = 3

@app.route('/api/batch/status', methods=['GET'])
def get_batch_status():
    """
    L·∫•y tr·∫°ng th√°i batch hi·ªán t·∫°i: s·ªë l∆∞·ª£ng comments v√† th·ªùi gian c√≤n l·∫°i
    
    Response:
    {
        "success": true,
        "current_count": 45,
        "batch_size": 128,
        "remaining_count": 83,
        "last_batch_time": "2026-01-17T10:30:00",
        "time_elapsed_seconds": 3600,
        "time_remaining_seconds": 7200,
        "will_trigger_at": "128 comments OR 2026-01-17T13:30:00"
    }
    """
    try:
        from datetime import datetime, timedelta
        
        # L·∫•y s·ªë l∆∞·ª£ng comments trong buffer
        current_count = redis_client.llen("absa:buffer")
        remaining_count = max(0, BATCH_SIZE - current_count)
        
        # L·∫•y th·ªùi gian batch cu·ªëi
        last_batch_str = redis_client.get("absa:last_batch_time")
        
        if last_batch_str:
            last_batch_time = datetime.fromisoformat(last_batch_str)
            time_elapsed = datetime.now() - last_batch_time
            time_remaining = timedelta(hours=BATCH_TIMEOUT_HOURS) - time_elapsed
            time_remaining_seconds = max(0, int(time_remaining.total_seconds()))
            next_trigger_time = last_batch_time + timedelta(hours=BATCH_TIMEOUT_HOURS)
        else:
            last_batch_time = None
            time_elapsed = timedelta(0)
            time_remaining_seconds = BATCH_TIMEOUT_HOURS * 3600
            next_trigger_time = datetime.now() + timedelta(hours=BATCH_TIMEOUT_HOURS)
        
        return jsonify({
            "success": True,
            "current_count": current_count,
            "batch_size": BATCH_SIZE,
            "remaining_count": remaining_count,
            "last_batch_time": last_batch_time.isoformat() if last_batch_time else None,
            "time_elapsed_seconds": int(time_elapsed.total_seconds()),
            "time_remaining_seconds": time_remaining_seconds,
            "will_trigger_at": f"{BATCH_SIZE} comments OR {next_trigger_time.strftime('%Y-%m-%d %H:%M:%S')}"
        }), 200
        
    except Exception as e:
        logger.error(f"‚ùå Error getting batch status: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500


@app.route('/api/batch/fill', methods=['POST'])
def fill_batch():
    """
    Fill batch v·ªõi filler comments ƒë·ªÉ force trigger inference.
    Spring Boot g·ªçi API n√†y ƒë·ªÉ ƒë·∫©y ƒë·ªß 128 comments.
    
    Request:
    {
        "filler_text": "FILLER_COMMENT_DO_NOT_PROCESS",  // N·ªôi dung filler (ƒë·ªÉ l·ªçc k·∫øt qu·∫£ sau)
        "callback_url": "http://host.docker.internal:8080/api/callbacks/absa"  // optional
    }
    
    Response:
    {
        "success": true,
        "filled_count": 83,
        "total_count": 128,
        "message": "Batch filled and will trigger inference"
    }
    """
    try:
        data = request.get_json() or {}
        filler_text = data.get("filler_text", "FILLER_COMMENT_IGNORE")
        callback_url = data.get("callback_url")
        
        # T√≠nh s·ªë l∆∞·ª£ng c·∫ßn fill
        current_count = redis_client.llen("absa:buffer")
        fill_count = max(0, BATCH_SIZE - current_count)
        
        if fill_count == 0:
            return jsonify({
                "success": True,
                "filled_count": 0,
                "total_count": current_count,
                "message": "Batch already full, inference will trigger soon"
            }), 200
        
        # T·∫°o filler comments
        filler_ids = []
        for i in range(fill_count):
            filler_id = f"FILLER_{int(time.time() * 1000)}_{i}"
            message = {
                "id": filler_id,
                "comment_text": filler_text,
                "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S"),
                "callback_url": callback_url,
                "is_filler": True  # ƒê√°nh d·∫•u l√† filler
            }
            
            # G·ª≠i v√†o Kafka + Redis
            producer.send(TOPIC, message)
            redis_client.rpush("absa:buffer", json.dumps(message))
            filler_ids.append(filler_id)
        
        producer.flush()
        
        new_count = redis_client.llen("absa:buffer")
        
        logger.info(f"‚úÖ Filled batch with {fill_count} filler comments")
        
        return jsonify({
            "success": True,
            "filled_count": fill_count,
            "total_count": new_count,
            "filler_ids": filler_ids,
            "filler_text": filler_text,
            "message": "Batch filled and will trigger inference soon"
        }), 201
        
    except Exception as e:
        logger.error(f"‚ùå Error filling batch: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500


# === Kh·ªüi ƒë·ªông ===
if __name__ == '__main__':
    try:
        init_kafka()
        logger.info("üöÄ Starting Comment API on port 5000...")
        app.run(host='0.0.0.0', port=5000, debug=False)
    except Exception as e:
        logger.error(f"‚ùå Failed to start API: {str(e)}")
        raise
